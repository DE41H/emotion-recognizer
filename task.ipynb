{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers, optimizers, callbacks # type: ignore\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './ravdess'\n",
    "SAMPLE_RATE = 22050\n",
    "N_MELS = 256\n",
    "DURATION = 3\n",
    "LENGTH = int(DURATION * SAMPLE_RATE)\n",
    "NOISE_FACTOR = 0.001\n",
    "STRETCH_FACTOR = 1.2\n",
    "SHRINK_FACTOR = 0.8\n",
    "SHIFT_FACTOR = 0.2\n",
    "CUT_LENGTH = 4000\n",
    "FREQ_MASK = 25\n",
    "TIME_MASK = 30\n",
    "EMOTIONS = {\n",
    "    '01': 0,\n",
    "    '02': 1,\n",
    "    '03': 2,\n",
    "    '04': 3,\n",
    "    '05': 4,\n",
    "    '06': 5,\n",
    "    '07': 6,\n",
    "    '08': 7\n",
    "}\n",
    "\n",
    "x_train, x_test, x_val = [], [], []\n",
    "y_train, y_test, y_val = [], [], []\n",
    "g_train, g_test, g_val = [], [], []\n",
    "a_train, a_test, a_val = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c879fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    data, _ = lb.load(path, sr=SAMPLE_RATE, duration=None)\n",
    "    data, _ = lb.effects.trim(data, top_db=30)\n",
    "    data = fix(data)\n",
    "    return data\n",
    "\n",
    "def fix(data):\n",
    "    if len(data) < LENGTH:\n",
    "        padding = int(LENGTH - len(data))\n",
    "        data = np.pad(data, (0, padding), 'constant')\n",
    "    elif len(data) > LENGTH:\n",
    "        data = data[:LENGTH]\n",
    "    return data\n",
    "\n",
    "def extract(data):\n",
    "    graph = lb.feature.melspectrogram(y=data, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    graph = lb.power_to_db(graph, ref=np.max)\n",
    "    delta = lb.feature.delta(graph)\n",
    "    graph = np.stack([graph, delta], axis=-1)\n",
    "    return graph.astype(np.float32)\n",
    "\n",
    "def spec_augument(graph):\n",
    "    spec = graph.copy()\n",
    "    n_mels = spec.shape[0]\n",
    "    f = np.random.randint(0, FREQ_MASK)\n",
    "    f0 = np.random.randint(0, n_mels - f)\n",
    "    spec[f0:f0 + f, :, :] = 0\n",
    "    n_time = spec.shape[1]\n",
    "    t = np.random.randint(0, TIME_MASK)\n",
    "    t0 = np.random.randint(0, n_time - t)\n",
    "    spec[:, t0:t0 + t, :] = 0\n",
    "    return spec\n",
    "\n",
    "def augument(data):\n",
    "    full_data = []\n",
    "    full_data.append(data)\n",
    "    noise = np.random.randn(LENGTH)\n",
    "    data_noise = np.array(data + NOISE_FACTOR * noise)\n",
    "    full_data.append(data_noise)\n",
    "    data_deep = lb.effects.pitch_shift(data, sr=SAMPLE_RATE, n_steps=-3)\n",
    "    full_data.append(data_deep)\n",
    "    data_shrill = lb.effects.pitch_shift(data, sr=SAMPLE_RATE, n_steps=+3)\n",
    "    full_data.append(data_shrill)\n",
    "    data_fast = lb.effects.time_stretch(data, rate=STRETCH_FACTOR)\n",
    "    data_fast = fix(data_fast)\n",
    "    full_data.append(data_fast)\n",
    "    data_slow = lb.effects.time_stretch(data, rate=SHRINK_FACTOR)\n",
    "    data_slow = fix(data_slow)\n",
    "    full_data.append(data_slow)\n",
    "    shift = np.random.randint(int(SAMPLE_RATE * SHIFT_FACTOR))\n",
    "    data_shift = np.roll(data, shift)\n",
    "    if shift > 0:\n",
    "        data_shift[:shift] = 0\n",
    "    data_shift = fix(data_shift)\n",
    "    full_data.append(data_shift)\n",
    "    gain_factor = np.random.uniform(0.8, 1.2)\n",
    "    data_gain = data * gain_factor\n",
    "    full_data.append(data_gain)\n",
    "    data_cut = data.copy()\n",
    "    start = np.random.randint(0, LENGTH - CUT_LENGTH)\n",
    "    stop = start + CUT_LENGTH\n",
    "    data_cut[start:stop] = 0\n",
    "    full_data.append(data_cut)\n",
    "    return full_data\n",
    "\n",
    "def get_file_data():\n",
    "    files_data = []\n",
    "    for root, _, files in os.walk(PATH):\n",
    "        for file in files:\n",
    "            if not file.endswith('.wav'):\n",
    "                continue\n",
    "            args = file.removesuffix('.wav').split('-')\n",
    "            emotion = args[2]\n",
    "            actor = args[6]\n",
    "            gender = 0 if int(actor) % 2 == 0 else 1\n",
    "            if emotion not in EMOTIONS:\n",
    "                continue\n",
    "            files_data.append({\n",
    "                'path': os.path.join(root, file),\n",
    "                'label': EMOTIONS[emotion],\n",
    "                'actor': int(actor),\n",
    "                'gender': gender\n",
    "            })\n",
    "    return files_data\n",
    "\n",
    "def parse(file_data, arr_x, arr_y, arr_g, arr_a):\n",
    "    data = load(file_data['path'])\n",
    "    graph = extract(data)\n",
    "    arr_x.append(graph)\n",
    "    arr_y.append(file_data['label'])\n",
    "    arr_a.append(file_data['actor'])\n",
    "    arr_g.append(file_data['gender'])\n",
    "\n",
    "def parse_and_augument(file_data, arr_x, arr_y, arr_g, arr_a):\n",
    "    data = load(file_data['path'])\n",
    "    full_data = augument(data)\n",
    "    for item in full_data:\n",
    "        graph = extract(item)\n",
    "        arr_x.append(graph)\n",
    "        arr_x.append(spec_augument(graph))\n",
    "        for _ in range(2):\n",
    "            arr_y.append(file_data['label'])\n",
    "            arr_a.append(file_data['actor'])\n",
    "            arr_g.append(file_data['gender'])\n",
    "        \n",
    "def save():\n",
    "    x = {\n",
    "        'train': np.array(x_train, dtype=np.float32),\n",
    "        'test': np.array(x_test, dtype=np.float32),\n",
    "        'val': np.array(x_val, dtype=np.float32),\n",
    "    }\n",
    "    y = {\n",
    "        'train': np.array(y_train, dtype=np.int8),\n",
    "        'test': np.array(y_test, dtype=np.int8),\n",
    "        'val': np.array(y_val, dtype=np.int8),\n",
    "    }\n",
    "    a = {\n",
    "        'train': np.array(a_train, dtype=np.int8),\n",
    "        'test': np.array(a_test, dtype=np.int8),\n",
    "        'val': np.array(a_val, dtype=np.int8),\n",
    "    }\n",
    "    g = {\n",
    "        'train': np.array(g_train, dtype=np.int8),\n",
    "        'test': np.array(g_test, dtype=np.int8),\n",
    "        'val': np.array(g_val, dtype=np.int8),\n",
    "    }\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    np.savez_compressed('data/dataset.npz', x_train=x['train'], x_test=x['test'], x_val=x['val'], \n",
    "                        y_train=y['train'], y_test=y['test'], y_val=y['val'], \n",
    "                        g_train=g['train'], g_test=g['test'], g_val=g['val'], \n",
    "                        a_train=a['train'], a_test=a['test'], a_val=a['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53637c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_data = get_file_data()\n",
    "for file_data in files_data:\n",
    "    if file_data['actor'] <= 18:\n",
    "        parse_and_augument(file_data, x_train, y_train, g_train, a_train)\n",
    "    elif file_data['actor'] <= 21:\n",
    "        parse(file_data, x_test, y_test, g_test, a_test)\n",
    "    else:\n",
    "        parse(file_data, x_val, y_val, g_val, a_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315fd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = int(input(\"1 => Train\\n2 => Test\\n3 => Train + Test\\n\\n: \"))\n",
    "\n",
    "PATH = './data'\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "EMOTIONS = ('Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgust', 'Surprised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    ds = np.load(os.path.join(PATH, 'dataset.npz'))\n",
    "    x_train, x_test, x_val = ds['x_train'], ds['x_test'], ds['x_val']\n",
    "    y_train, y_test, y_val = ds['y_train'], ds['y_test'], ds['y_val']\n",
    "    a_train, a_test, a_val = ds['a_train'], ds['a_test'], ds['a_val']\n",
    "    g_train, g_test, g_val = ds['g_train'], ds['g_test'], ds['g_val']\n",
    "    if x_train.ndim == 3:\n",
    "        x_train = np.expand_dims(x_train, axis=-1)\n",
    "    if x_test.ndim == 3:\n",
    "        x_test = np.expand_dims(x_test, axis=-1)\n",
    "    if x_val.ndim == 3:\n",
    "        x_val = np.expand_dims(x_val, axis=-1)\n",
    "    return (x_train, x_test, x_val), (y_train, y_test, y_val), (a_train, a_test, a_val), (g_train, g_test, g_val)\n",
    "\n",
    "def init(shape):\n",
    "    inputs = layers.Input(shape=shape)\n",
    "    x = layers.Normalization(axis=-1)(inputs)\n",
    "\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.SpatialDropout2D(0.1)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.SpatialDropout2D(0.1)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "    gap = layers.GlobalAveragePooling2D()(x)\n",
    "    gmp = layers.GlobalMaxPooling2D()(x)\n",
    "    x = layers.Concatenate()[gap, gmp]\n",
    "\n",
    "    x = layers.Dense(256, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(128, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(8, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    loss = 'sparse_categorical_crossentropy'\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(model, x_train, x_val, y_train, y_val):\n",
    "    checkpoint = callbacks.ModelCheckpoint('data/weights.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    dynamic_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=9, min_lr=0.000001, verbose=1)\n",
    "    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "    weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    weights_dict = dict(enumerate(weights))\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[checkpoint, dynamic_lr, early_stop],\n",
    "        verbose=1,\n",
    "        class_weight=weights_dict\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def test(x_test, y_test, g_test):\n",
    "    print(f'Testing Model....')\n",
    "    model = models.load_model('data/weights.keras')\n",
    "    predictions = model.predict(x_test)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"-\" * 50)\n",
    "    print(classification_report(y_test, y_pred, target_names=EMOTIONS))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"CONFUSION MATRIX (Text)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(cm)\n",
    "    female_idx = np.where(g_test == 0)[0]\n",
    "    male_idx = np.where(g_test == 1)[0]\n",
    "    female_acc = np.mean(y_pred[female_idx] == y_test[female_idx])\n",
    "    male_acc = np.mean(y_pred[male_idx] == y_test[male_idx])\n",
    "    print('\\n' + '-' * 50)\n",
    "    print('PITCH/GENDER BIAS ANALYSIS')\n",
    "    print('-' * 50)\n",
    "    print(f'Female Accuracy: {female_acc*100:.2f}%')\n",
    "    print(f'Male Accuracy: {male_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_test, x_val), (y_train, y_test, y_val), (a_train, a_test, a_val), (g_train, g_test, g_val) = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if var == 1 or var == 3:\n",
    "    model = init(x_train.shape[1:])\n",
    "    model.layers[0].adapt(x_train)\n",
    "    history = train(model, x_train, x_val, y_train, y_val)\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if var == 2 or var == 3:\n",
    "    test(x_test, y_test, g_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
